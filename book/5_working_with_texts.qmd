---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Работа с текстами

Не существует какого-то единого алгоритма анализа текстов, многое зависит от задач. Однако все обычные сферы анализа данных применимы к текстам: иногда нужно выделить какие-то составляющие текста (частота встречаемости каких-то единиц, сентимент анализ), иногда нужно выделить уникальные единицы свойственные какой-то группе текстов (мера tf-idf), иногда нужно кластеризовать тексты, чтобы найти похожие/разные тексты (например, класическая задача определения спамерских сообщений). Такие задачи находятся на стыке лингвистики и компьютерных наук. Существуют и более сложные/интеллектуальные задачи, которые традиционно относят к области исскуственного интеллекта, такие как перевод с одного языка на другой, саммаризация текста, вопросно-ответные системы и другие. В данном разделе мы коснемся лишь первой группы задач и пакетов, написанных на R для их решения.

## Загрузка текстов в R {#sec-encoding}

```{r}
#| message: false
library(tidyverse)
```

В пакете `readr` (входит в `tidyverse`) для чтения текста есть функция `read_lines()`. В качестве первой переменной может выступать путь к файлу на компьютере или интернет ссылка:

```{r}
alices_adventures_in_wonderland <- read_lines("https://raw.githubusercontent.com/agricolamz/daR4hs/main/data/w5_alices_adventures_in_wonderland.txt")
head(alices_adventures_in_wonderland, 20)
```

В большинстве случаев, тексты получится считать, однако иногда при работе со старыми архивами могут возникнуть проблемы с кодировками, например, все тексты в старейшей интернет-библиотеке на русском языке --- библиотеке Максима Машкова ([lib.ru](lib.ru)) --- записаны в `KOI8-R`:

```{r}
read_lines("https://raw.githubusercontent.com/agricolamz/daR4hs/main/data/w5_the_captains_daughter_koi8r.txt",
           n_max = 15)
```

В функциях пакета `readr` (т. е. не только в функции `read_lines()`, но и в функциях `read_csv()`, `read_tsv()` и т. п.) есть аргумент `locale`, который позволяет эксплицитно указать кодировку, а при считывании происходит процесс конвертации в стандартный для многих операционных систем `UTF-8`. Для текстов на русском языке важны следующие кодировки

- `KOI8-R`, а для украинского языка --- `KOI8-U`;
- `CP1251` (также известная под названием `Windows-1251`) покрывает и другие кириллические письменности такие как украинский, белорусский, болгарский, сербский, македонский и другие.

```{r}
read_lines("https://raw.githubusercontent.com/agricolamz/daR4hs/main/data/w5_the_captains_daughter_koi8r.txt",
           locale = locale(encoding = "KOI8-R"), 
           n_max = 15)

read_lines("https://raw.githubusercontent.com/agricolamz/daR4hs/main/data/w5_the_captains_daughter_cp1251.txt",
           locale = locale(encoding = "CP1251"),
           n_max = 15)
```

Для просмотра и изменения кодировки внутри R следует использовать функцию `Encoding()`:

```{r}
x <- "fa\xE7ile"
x
Encoding(x)
```

Теперь можем использовать функцию присваивания:

```{r}
Encoding(x) <- "latin1"
x
Encoding(x)
```

Если необходимо преобразовать из одной кодировки в другую, следует использовать функцию `iconv()`:

```{r}
x <- iconv(x, "latin1", "UTF-8")
Encoding(x)
x
```

## Библиотеки текстов


### Пакет `gutenbergr`

Пакет `gutenbergr` является API для очень старого [проекта Gutenberg](http://www.gutenberg.org/).

```{r}
library(gutenbergr)
```

Все самое важное в пакете хранится во встроенном датасете `gutenberg_metadata`:

```{r}
str(gutenberg_metadata)
```

Например, сейчас мы можем понять, сколько книг на разных языках можно скачать из проекта:

```{r}
gutenberg_metadata |> 
  count(language, sort = TRUE)
```

Как видно, в основном это тексты на английском. Сколько авторов в датасете?

```{r}
gutenberg_metadata |> 
  count(author, sort = TRUE)
```

Сколько произведений Джейн Остин (не перепутайте с другими Остин) есть в датасете?

```{r}
gutenberg_metadata |> 
  filter(author == "Austen, Jane") |> 
  distinct(gutenberg_id, title)
```

Давайте скачаем "Эмму":

```{r download_emma}
#| cache: true

emma <- gutenberg_download(158)
emma
```

Можно скачивать сразу несколько книг. Давайте добавим еще "Леди Сьюзен":

```{r download_books}
#| cache: true

books <- gutenberg_download(c(158, 946), meta_fields = "title")
books
books |> 
  count(title)
```

Обратите внимание на аргумент `meta_fields`, который позволяет кроме самого текста добавить метаданные из `gutenberg_metadata` в получившийся датафрейм.

### Пакет `rperseus`

Пакет `rperseus` предоставляет API для текстов  [Perseus Digital Library](http://www.perseus.tufts.edu/), где хранятся тексты на греческом и латинском языках. 

На момент создания этих материалов пакет не был опубликован в CRAN, так что для его установки нужно выполнить следующую команду (если в вашей системе не стоит пакета `remotes`, то его можно установить при помощи стандартных `install.packages("remotes")`):

```{r}
#| eval: false

remotes::install_github("ropensci/rperseus")
```

Загружаем библиотеку:

```{r}
library(rperseus)
```

Библиотечный каталог пакета `rperseus` находится в переменной `perseus_catalog`:

```{r}
str(perseus_catalog)
```

На каких языках содержатся тексты?

```{r}
perseus_catalog |> 
  count(language)
```

В документации объясняется:

- `grc` --- греческий;
- `lat` --- латинский;
- `eng` --- английский;
- `hpt` --- иврит с огласовками;
- `hct` --- иврит без огласовок;
- `ger` --- немецкий;
- `oth` --- другие языки.

Чтобы скачивать тексты, нужно использовать индексы из переменной `urn`. Давайте скачаем "Исповедь" Августина:

```{r download_augustin}
#| cache: true

augustins_confession <- get_perseus_text("urn:cts:latinLit:stoa0040.stoa001.opp-lat1")
augustins_confession
```

Кроме того, можно скачать лишь фрагмент, если мы заранее знаем деление книги:

```{r download_augustin_excerpt}
#| cache: true

get_perseus_text("urn:cts:latinLit:stoa0040.stoa001.opp-lat1", excerpt = 4)
```

К сожалению, пакет не позволяет скачивать много текстов за раз, но в документации описано, как это можно сделать при помощи цикла. Кому-то может показаться полезной функция `perseus_parallel()`, которая позволяет видеть параллельные фрагменты текста. Проиллюстрируем это на примере Эвменид Эсхила:

```{r eumenides}
#| cache: true
#| warning: false

get_perseus_text("urn:cts:greekLit:tlg0085.tlg007.perseus-eng2") |> 
  slice(1) ->
  aeschylus_eumenides_eng

get_perseus_text("urn:cts:greekLit:tlg0085.tlg007.perseus-grc2") |> 
  slice(1) ->
  aeschylus_eumenides_grc

aeschylus_eumenides_eng |> 
  bind_rows(aeschylus_eumenides_grc) |> 
  perseus_parallel()
```

### Библиотека lib.ru

Для текстов на русском языке отдельного пакета не написали, однако на сайте библиотеки [lib.ru](lib.ru) они уже представлены в машиночитаемом виде, нужно всего лишь выбрать вариант отображения `txt`, полученную ссылку считать в R, указав корректную кодировку (см. @sec-encoding):

```{r}
read_lines("http://lib.ru/LITRA/PUSHKIN/kapitan.txt_Ascii.txt",
           locale = locale(encoding = "KOI8-R"),
           n_max = 15)
```

Если вам нужно скачать весь текст, разумеется, аргумент `n_max` нужно убрать, он отвечает за количество скачанных строк.

## Библиотека `tidytext`

Сейчас скачанные тексты записывались в таблицу, где одна строка содержала один абзац. Однако для анализа текста нужно уметь работать с отдельными словами в нем. Для этого тексты нужно привести в tidy формат. С этим отлично справляется пакет `tidytext` (онлайн книга доступна [здесь](https://www.tidytextmining.com/)). Основное "оружие" пакета `tidytext` функция `unnest_tokens()`, которая переводит текст в tidy формат. В аргумент `output` подается вектор с именем будущей переменной, а аргумент `input` принимает переменную с текстом.

```{r}
library(tidytext)
alices_adventures_in_wonderland |>  
  tibble(text = _) |> 
  unnest_tokens(output = "word", input = text)
```

По умолчанию функция `unnest_tokens()` удаляет знаки препинания и приводит слова к нижнему регистру. Давайте для удобства удалим все вплоть до оглавления и создадим датафрейм и переменную, из которой будет понятно, где какая глава книги:

```{r}
alices_adventures_in_wonderland |> 
  tibble(text = _) |> 
  slice(54:3405) |> 
  mutate(chapter = str_extract(text, "CHAPTER .{1,4}\\."),
         chapter_title = ifelse(str_detect(text, "CHAPTER .{1,4}\\."), lead(text), NA)) |> 
  fill(chapter, chapter_title)  |> 
  filter(text != chapter) |> 
  mutate(chapter = str_remove(chapter, "CHAPTER "),
         chapter = str_remove(chapter, "\\."),
         chapter = as.numeric(as.roman(chapter)),
         chapter_title = fct_reorder(chapter_title, chapter)) ->
  alice_cleaned

alice_cleaned
```

В приведенном выше коде интерес представляет функция `fill()`, которая заполнила пропущенные значения `NA` значениями выше. В остальном мы использовали несложные регулярные выражения из @sec-regex. Кроме того, мы воспользовались встроенными функциями `as.roman()` и `as.numeric()`, чтобы преобразовать римские номера глав в арабские. Теперь мы готовы анализировать частотность слов. Создадим переменную `tidy_alice` и посчитаем слова в каждой из глав:

```{r}
alice_cleaned |> 
  unnest_tokens(output = "word", input = text) ->
  tidy_alice

tidy_alice |> 
  count(chapter_title, word, sort = TRUE)
```

В основном, конечно, слова бессмысленные, но давайте посмотрим первый десяток самых частотных слов в каждой из глав:

```{r}
tidy_alice |> 
  count(chapter_title, word, sort = TRUE) |> 
  group_by(chapter_title) |> 
  slice(1:10) |> 
  ggplot(aes(n, word))+
  geom_col()+
  facet_wrap(~chapter_title, scales = "free")+
  labs(x = NULL, y = NULL)
```

Хорошие знатоки "Алисы в Зазеркалье", конечно, многое понимают из распределения местоимений *you* и, наверное, догадываются, почему это слово становится первым, однако в большинстве случаев служебные слова неинформативны при анализе текстов. Поэтому для некоторых языков составили списки стоп-слов --- служебных слов, которые никак не приближают к пониманию текста. Отдельный список уже встроен в пакет `tidytext` в переменную `stop_words` (см. также следующий раздел):

```{r}
stop_words
```

Так что используя функцию из семейства `..._join()` (см. @sec-joins), мы можем повторить наш анализ:

```{r}
tidy_alice |> 
  count(chapter_title, word, sort = TRUE) |> 
  anti_join(stop_words) |> 
  group_by(chapter_title) |> 
  slice(1:10) |> 
  ggplot(aes(n, word))+
  geom_col()+
  facet_wrap(~chapter_title, scales = "free")+
  labs(x = NULL, y = NULL)
```

Получившийся результат уже значительно интереснее, однако алфавитный порядок слов в каждом фасете немного мешает. Для того, чтобы победить это, в пакете `tidytext` есть несколько функций: 

- `reorder_within()` --- функция которая позволяет указать группировку, внутри которой нужно упорядочить единицы;
- `scale_y_reordered()`/`scale_x_reordered()` --- функция, которая маскирует работу `reorder_within()` при отображении на графике.
 
```{r}
tidy_alice |> 
  count(chapter_title, word, sort = TRUE) |> 
  anti_join(stop_words) |> 
  group_by(chapter_title) |> 
  slice(1:10) |> 
  mutate(word = reorder_within(word, within = chapter_title, by = n)) |> 
  ggplot(aes(n, word))+
  geom_col()+
  facet_wrap(~chapter_title, scales = "free")+
  scale_y_reordered()+
  labs(x = NULL, y = NULL)
```

Важно отметить, что разделение на слова не единственная цель функции `unnest_tokens()`, важным бывает анализировать сочетания слов: биграммы, триграммы и т. д.

```{r}
alice_cleaned |> 
  unnest_tokens(output = "ngram", input = text, token = "ngrams", n = 2)
```

В связи с этим можно повторить наш анализ с биграммами:

```{r}
alice_cleaned |> 
  unnest_tokens(output = "ngram", input = text, token = "ngrams", n = 2) |> 
  na.omit() |> 
  separate(ngram, into = c("ngram_1", "ngram_2"), sep = " ") |> 
  anti_join(tibble(ngram_1 = stop_words$word)) |> 
  anti_join(tibble(ngram_2 = stop_words$word)) |> 
  mutate(ngram = str_c(ngram_1, " ", ngram_2)) |> 
  count(chapter_title, ngram, sort = TRUE) |>
  group_by(chapter_title) |> 
  slice(1:10) |> 
  mutate(ngram = reorder_within(ngram, within = chapter_title, by = n)) |> 
  ggplot(aes(n, ngram))+
  geom_col()+
  facet_wrap(~chapter_title, scales = "free")+
  scale_y_reordered() + 
  labs(x = NULL, y = NULL)
```

## Пакет `stopwords`

Выше мы упомянули, что в пакет `tidytext` встроен список английских стоп-слов. Стоп-слова для других язков можно раздобыть списки для других языков, используя пакет `stopwords`. Вместо имени языка, функция принимает ISO код языыка:

```{r}
library(stopwords)
stopwords("ru")
```

Пакет предоставляет несколько источников списков:

```{r}
stopwords_getsources()
```

Давайте посмотрим какие языки сейчас доступны:

```{r}
map(stopwords_getsources(), stopwords_getlanguages)
```

Мы видим, что есть несколько источников для русского языка:

```{r}
length(stopwords("ru", source = "snowball"))
length(stopwords("ru", source = "stopwords-iso"))
length(stopwords("ru", source = "marimo"))
length(stopwords("ru", source = "nltk"))
```

В результате мы можем сделать анализ, аналогичный анализу Алисы из прошлого раздела, для текста на русском языке:

```{r}
captains_daughter <- read_lines("https://raw.githubusercontent.com/agricolamz/daR4hs/main/data/w5_the_captains_daughter_koi8r.txt",
                                locale = locale(encoding = "KOI8-R"))

captains_daughter |> 
  tibble(text = _) |> 
  unnest_tokens(output = "word", input = text) |> 
  anti_join(tibble(word = stopwords("ru", source = "stopwords-iso"))) |> 
  count(word, sort = TRUE) |> 
  slice(1:15) |> 
  mutate(word = fct_reorder(word, n)) |> 
  ggplot(aes(n, word))+
  geom_col()+
  labs(x = NULL, y = NULL, caption = "А. С. Пушкин 'Капитанская дочка'")
```

## Пакет `udpipe`

Пакет `udpipe` представляет лемматизацию, морфологический и синтаксический анализ разных языков. Туториал можно найти [здесь](https://bnosac.github.io/udpipe/docs/doc1.html), там же есть список доступных языков.

```{r}
library(udpipe)
```

Модели качаются очень долго.
```{r download_en_model}
#| cache: true

enmodel <- udpipe_download_model(language = "english")
```

Теперь можно распарсить какое-нибудь предложение:
```{r}
udpipe("The want of Miss Taylor would be felt every hour of every day.",
       object = enmodel)
```

Скачаем русскую модель:
```{r  download_ru_model}
#| cache: true
rumodel <- udpipe_download_model(language = "russian-syntagrus")
```

```{r}
udpipe("Жила-была на свете крыса в морском порту Вальпараисо, на складе мяса и маиса, какао и вина.",
       object = rumodel)
```

После того, как модель скачана, можно к ней обращаться просто по имени файла:

```{r}
udpipe("Жила-была на свете крыса в морском порту Вальпараисо, на складе мяса и маиса, какао и вина.",
       object = "russian-syntagrus-ud-2.5-191206.udpipe")
```

Функция `udipipe()` также принимает датафрейм со столбцами `doc_id` и `text` --- это позволяет сохранить в получившейся таблице структуру, которая важна в исследовании, так как в переменную `doc_id` можно спрятать всю необходимую информацию. 

## Векторизация текстов

Векторизация слов и текстов --- это операция, которая позволяет превратить наборы текстов в набор числовых значений, которые потом можно использовать для кластеризации текстов. Мы будем использовать для этой цели дефолтные значения достаточно старых моделей, которые мы используем в практике из-за их скорости. Датасет, который мы будем векторизовывать содержит 300 аннотаций из научных журналов по трем тематикам: лингвистике, медицине и экономике (каждой дисциплины по 100 аннотаций).

```{r}
abstracts_dataset <- read_csv("https://raw.githubusercontent.com/agricolamz/daR4hs/main/data/w5_abstracts.csv")
str(abstracts_dataset)
```

Мы будем использовать функцию `paragraph2vec()` из пакета `doc2vec`. Сначала мы приведем наш датасет к виду датафрейма с переменными `doc_id` и `text`.

```{r}
library(doc2vec)

abstracts_dataset |>
  mutate(doc_id = 1:n()) |>
  rename(text = abstract) |>
  select(doc_id, text) |> 
  paragraph2vec(threads = 15) ->
  model
```

Эмбединги из переменной `model` можно получить следующим образом:

```{r}
as.matrix(model, which = "docs") |> 
  dim()
```

Функция `dim()` говорит о размерности получившейся таблицы --- 300 аннотаций, 50 числовых значений. Теперь можно воспользоваться методами уменьшения размерности для того, чтобы 50-мерное пространство схлопнуть до 2-мерного. Существует много алгоритмов, которые это делают, я воспользуюсь алгоритмом UMAP (Uniform Manifold Approximation and Projection):

```{r}
library(uwot)
as.matrix(model, which = "docs") |>
  umap(metric = "cosine", init = "pca") |> 
  as_tibble() |> 
  bind_cols(abstracts_dataset) |> 
  ggplot(aes(V1, V2, color = field))+
  geom_point()+
  stat_ellipse()
```

Мы видим, что у аннотаций есть что-то общее, но, в общем, аннотации распадаются на три группы. Важно, что расцветка у нас уже была в датасете и алгоритм векторизации ее не видел, так что получившееся пространство отражает карту анализируемых текстов. Стоит отметить, что качество получившегося пространства напрямую зависит от объема данных.
