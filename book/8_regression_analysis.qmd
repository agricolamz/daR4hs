---
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r}
#| include: false

library(tidyverse)
theme_set(theme_bw())
```

```{r}
library(tidyverse)
```

# Основы статистического анализа

## О статистике

Статистика имеет большое значение в науке, и все чаще она позволяет принимать решения правительствам, корпорациям и т. п. В области принятия решений статистику сейчас теснят методы машинного обучения, однако в науке все же статистика продолжает занимать важное место, помогая подтвердить обнаруженные факты, принимать решения на основе данных и делать предсказания.

Важно подчеркнуть разницу между статистикой и машинным обучением: она заключается в основных целях. Статистика в основном занимается выводами и моделированием процессов, которые генерируют наблюдаемые данные, в то время как машинное обучение фокусируется на поиске обобщаемых предиктивных закономерностей в данных. Несмотря на то, что некоторые методы используются как в статистике, так и в машинном обучении, все же эти две области имеют разные цели и подходы.

Важно помнить, что статистика позволяет лишь моделировать процессы, которые стоят за генерацией данных. В связи с этим не стоит считать, что статистический вывод позволяет узнать правду или сделать правильный выбор: правильность и истинность любого конкретного случая зависит от научных установок исследователя, ответственного применения методов с соблюдением всех ограничений на их применение, качества сбора данных и теоретического обоснования процедуры анализа. Я призываю смотреть на статистику как на инструмент, который помогает в принятии решений, предсказании и моделировании: если вы забили шуруп молотком, то, вероятно, вашу задачу вы решили, но молоток сам по себе не несет ответственности за результат.

Сейчас широко распространены две школы статистического анализа: фреквентисткий (еще можно встретить термин *частотный*) и байесовский. Разница между этими подходами заключается в их основных принципах. Частотный подход основан на оценке статистического параметра на основе частоты его появления в выборке, в то время как байесовский подход позволяет представить оценку статистического параметра как распределение вероятностей, соединяя априорные знания о предметной области и новые данные. Фреквентистские методы получили широкое распространение в XX веке, а байесовские методы применялись задолго до фреквентистских, однако с развитием вычислительных мощностей в XXI веке популярность байесовских методов растет. В данной главе мы будем опираться на фреквентистские методы, так как погружение в байесовские методы требует значительной подготовки.

## Тест Стьюдента (t-тест)

Первый статистический метод, который обычно обсуждают в фреквентистской статистике --- это тест Стьюдента. Основное его применение заключается в сравнении средних значений двух групп (двухвыборочный тест) или среднего одной группы с некоторым значением (одновыборочный тест). Среди возможных задач, которые можно решить при помощи теста Стьюдента:

- оценить эффект нового лекарства по сравнению со старым;
- оценить эффект нового метода обучения на результатах тестов групп студентов;
- оценить отличие какого-нибудь параметра популяции (например, избыточного веса) по сравнению с нормой.

Теперь мы обсудим стандартную процедуру фреквентистской статистики. Допустим мы сравниваем средние двух групп А и B. Принято создавать две гипотезы:

* $H_0$ --- (нулевая гипотеза) разница между группами не является статистически значимой, т. е. наблюдаемые данные происходят из одного ожидаемого распределения.
* $H_1$ --- (альтернативная гипотеза) разница является статистически значимой, т. е. наблюдаемые данные не происходят из одного ожидаемого распределения.

Нулевая гипотеза --- это гипотеза, которую каждый исследователь в случае успеха отвергнет и примет альтернативную. После применения статистического критерия (каждый критерий зависит от конкретного статистического теста, а выбор теста зависит от типа данных) исследователь считает вероятность наблюдения такого или более экстремального результат, если верна нулевая гипотеза (**p-value, p-уровень значимости**).

В тесте Стьюдента таким распределением является t-распредление. Распределение, которое напоминает нормальное распределение, но имеет более широкие края:

```{r}
#| echo: false
tibble(x = seq(-7, 7, by = 0.01),
       y = dt(x, df = 29)) |> 
  ggplot(aes(x, y))+
  geom_line()+
  labs(title = "t-распределение с параметром df = 29")
```

T-распределение имеет один параметр, который принято называть степенями свободы (degree of freedom, df). Чем больше степеней свободы, тем больше t-распределение похоже на нормальное распределение. Обычно этот параметр напрямую связывают с количеством наблюдений.

```{r}
tibble(x = rep(seq(-7, 7, by = 0.01), 4),
       df = rep(c(5, 15, 30, 75), 1401),
       y = dt(x, df =df)) |> 
  ggplot(aes(x, y, color = as.factor(df)))+
  geom_line() + 
  labs(color = "")
```

Давайте разберем на примере. Из статьи [@stepanova11] мы знаем, что носители русского языка в среднем говорят 5.31 слога в секунду со стандартным отклонением 1,93 (мужчины 5.46 слога в секунду со средним отклонением 2.02, женщины 5.23 слога в секунду со средним отклонением 1.84, дети 3.86 слога в секунду со средним отклонением 1.67). Мы опросили 30 носителей деревни N и выяснили, что среднее равно 7, а стандартное отклонение равно 2. Является ли данная разница статистически значимой? Рассмотрим данные

```{r}
set.seed(42)

data <- rnorm(n = 30, mean = 7, sd = 2)

data
```

Вот так можно визуализировать наш вопрос:

```{r}
#| warning: false

tibble(data) |> 
  ggplot(aes(data))+
  geom_dotplot()+
  geom_vline(xintercept = mean(data), size = 2, linetype = 2)+
  geom_vline(xintercept = 5.31, size = 2, linetype = 2, color = "red")+
  annotate(geom = "text", x = 3, color = "red", y = 0.75, label = "среднее согласно\n[Stepanova 2011]", size = 5)
```

Применяем тест Стьюдента. Так как мы сравниваем нашу выборку с некоторым эталонным значением, мы применяем одновыборочный тест Стьюдента, а эталонное значение записываем в аргумент `mu`:

```{r}
t.test(data, mu = 5.31)
```

Если мы хотим визуализировать t-статистику, которую мы получили на t-распределении, получится вот такой график:

```{r}
tibble(x = seq(-7, 7, by = 0.01),
       y = dt(x, df = 29)) |> 
  ggplot(aes(x, y))+
  geom_line()+
  geom_vline(linetype = 2, xintercept = 3.9871)+
  labs(title = "t-распределение с параметром df = 29")
```

Если бы значение t-критерия оказалось где-то посередине, то тест Стьюдента бы выдал p-value больше 0.05 и тогда у нас бы не было оснований ни для отвержения, ни для принятия нулевой гипотезы. 

### Двухвыборочный тест Стьюдента

Логика двухвыборочного теста идентична логике одновыброчного, с тем лишь исключением, что мы сравниваем две группы: 

```{r}
#| message: false
#| echo: false

set.seed(42)
sample_1 <- rnorm(25, mean = 40, sd = 5)
sample_2 <- rnorm(25, mean = 50, sd = 4.5)

tibble(sample_1, sample_2) |> 
  pivot_longer(names_to = "dataset", values_to = "values", sample_1:sample_2) |> 
  group_by(dataset) |> 
  mutate(mean = mean(values)) |> 
  ggplot(aes(values, fill = dataset))+
  geom_dotplot(show.legend = FALSE)+
  geom_vline(aes(xintercept = mean, color = dataset), size = 2, linetype = 2, show.legend = FALSE)

t.test(sample_1, sample_2)
```

На практике разница видна только в названии аргументов. Представим, что у нас есть две выборки:

```{r}
sample_1
sample_2
```

Тогда для того, чтобы применить тест Стьюдента, нужно запустить следующий код:

```{r}
t.test(sample_1, sample_2)
```

## Регрессионный анализ

### Основы

Суть регрессионного анализа заключается в моделировании связи между двумя и более переменными при помощи прямой на плоскости. Формула прямой зависит от двух параметров: свободного члена (intercept) и углового коэффициента (slope).

```{r}
#| echo: false

set.seed(42)
tibble(x = rnorm(100)+1,
       y = rnorm(100)+3) |> 
  ggplot(aes(x, y))+
  geom_point(alpha = 0)+
  geom_hline(yintercept = 0)+
  geom_vline(xintercept = 0)+
  geom_abline(intercept = 1, slope = 1, color = "red")+
  geom_abline(intercept = 2, slope = 2, color = "darkgreen")+
  geom_abline(intercept = 3, slope = -1, color = "navy")+
  scale_x_continuous(breaks = -2:4)+
  scale_y_continuous(breaks = 0:5)+
  coord_equal()
```

Когда мы пытаемся научиться предсказывать данные одной переменной $Y$ при помощи другой переменной $X$, мы получаем похожую формулу:

$$y_i = \hat\beta_0 + \hat\beta_1 \times x_i + \epsilon_i,$$
где

* $x_i$ --- $i$-ый элемент вектора значений $X$;
* $y_i$ --- $i$-ый элемент вектора значений $Y$;
* $\hat\beta_0$ --- оценка случайного члена (intercept);
* $\hat\beta_1$ --- оценка углового коэффициента (slope);
* $\epsilon_i$ --- $i$-ый остаток, разница между оценкой модели ($\hat\beta_0 + \hat\beta_1 \times x_i$) и реальным значением $y_i$; весь вектор остатков иногда называют случайным шумом (на графике выделены красным).

```{r}
#| message: false
#| warning: false
#| echo: false  

set.seed(42)
tibble(x = rnorm(30, mean = 50, sd = 10), 
       y = x + rnorm(30, sd = 10)) |> 
  mutate(x = x - mean(x)+ 50,
         y = y - mean(y)+ 55) ->
  df

coef <- round(coef(lm(y~x, data = df)), 3)
df |> 
  ggplot(aes(x, y))+
  geom_point(size = 2)+
  geom_smooth(se = FALSE, method = "lm")+
  annotate(geom = "label", x = 35, y =70, size = 5,
           label = latex2exp::TeX(str_c("$y_i$ = ", coef[1], " + ", coef[2], "$\\times x_i + \\epsilon_i$")))+
  geom_segment(aes(xend = x, yend = predict(lm(y~x, data = df))), color = "red", linetype = 2)
```

Задача регрессии --- оценить параметры $\hat\beta_0$ и $\hat\beta_1$, если нам известны все значения $x_i$ и $y_i$ и мы пытаемся минимизировать значения $\epsilon_i$. В данном конкретном случае, задачу можно решить аналитически и получить следующие формулы:

$$\hat\beta_1 = \frac{(\sum_{i=1}^n x_i\times y_i)-n\times\bar x \times \bar y}{\sum_{i = 1}^n(x_i-\bar x)^2}$$

$$\hat\beta_0 = \bar y - \hat\beta_1\times\bar x$$

### Первая регрессия

Давайте попробуем смоделировать количество слов *и* в рассказах М. Зощенко в зависимости от длины рассказа:
```{r}
#| message: false
zoshenko <- read_csv("https://raw.githubusercontent.com/agricolamz/daR4hs/main/data/w8_tidy_zoshenko.csv")

zoshenko |> 
  filter(word == "и") |> 
  distinct() ->
  zoshenko_filtered

zoshenko_filtered |>   
  ggplot(aes(n_words, n))+
  geom_point()+
  labs(x = "количество слов в рассказе",
       y = "количество и")
```

Давайте избавимся от них и добавим регрессионную линию при помощи функции `geom_smooth()`:

```{r}
#| message: false

zoshenko_filtered |> 
  ggplot(aes(n_words, n))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)+
  labs(x = "количество слов в рассказе",
       y = "количество и")
```

Чтобы получить формулу этой линии нужно запустить функцию, которая оценивает линейную регрессию:

```{r}
fit <- lm(n~n_words, data = zoshenko_filtered)
fit
```

Вот мы и получили коэффициенты, теперь мы видим, что наша модель считает следующее:

$$n = -1.47184 + 0.04405 \times n\_words$$

Более подробную информацию можно посмотреть, если запустить модель в функцию `summary()`:

```{r}
summary(fit)
```

В разделе `Coefficients` содержится информацию про наши коэффициенты: 

* `Estimate` -- полученная оценка коэффициентов;
* `Std. Error` -- стандартная ошибка среднего;
* `t value` -- $t$-статистика, полученная при проведении одновыборочного $t$-теста, сравнивающего данный коэфициент с 0;
* `Pr(>|t|)` -- полученное $p$-значение;
* `Multiple R-squared` и	`Adjusted R-squared` --- одна из оценок модели, показывает связь между переменными. Без поправок совпадает с квадратом коэффициента корреляции Пирсона.
* `F-statistic` --- $F$-статистика полученная при проведении теста, проверяющего, не является ли хотя бы один из коэффицинтов статистически значимо отличным от нуля. Совпадает с результатами дисперсионного анализа (ANOVA).

Теперь мы можем даже предсказывать значения, которые мы еще не видели. Например, сколько будет *и* в рассказе Зощенко длиной 1000 слов?

```{r}
#| echo: false
#| message: false

pr <- predict(fit, tibble(n_words = 1000))
zoshenko_filtered |>   
  ggplot(aes(n_words, n))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)+
  labs(x = "количество слов в рассказе М. Зощенко",
       y = "количество и")+
  annotate(geom = "segment", x = 1000, xend = 1000, y = -Inf, yend = pr, 
           linetype = 2, color = "red")+
  annotate(geom = "segment", x = 1000, xend = 0, y = pr, yend = pr, 
           linetype = 2, color = "red", arrow = arrow(type = "closed", length = unit(0.2, "inches")))+
  scale_y_continuous(breaks = round(c(1:3*20, unname(pr)), 2))
```

```{r}
predict(fit, tibble(n_words = 1000))
```

### Категориальные переменные

Что если мы хотим включить в наш анализ категориальные переменные? Давайте рассмотрим простой пример с [рассказами Чехова](https://raw.githubusercontent.com/agricolamz/daR4hs/main/data/w8_tidy_chekhov.csv) и [Зощенко](https://raw.githubusercontent.com/agricolamz/daR4hs/main/data/w8_tidy_zoshenko.csv), которые мы рассматривали в прошлом разделе. Мы будем анализировать логарифм доли слова *деньги*:

```{r}
#| message: false
chekhov <- read_csv("https://raw.githubusercontent.com/agricolamz/daR4hs/main/data/w8_tidy_chekhov.csv")
zoshenko <- read_csv("https://raw.githubusercontent.com/agricolamz/daR4hs/main/data/w8_tidy_zoshenko.csv")

chekhov |> 
  bind_rows(zoshenko) |> 
  filter(str_detect(word, "деньг")) |> 
  group_by(author, titles, n_words) |> 
  summarise(n = sum(n)) |> 
  mutate(log_ratio = log(n/n_words)) ->
  checkov_zoshenko
```

Визуализация выглядит так:

```{r}
#| echo: false

checkov_zoshenko |> 
  group_by(author) |> 
  mutate(mean = mean(log_ratio)) |> 
  ggplot(aes(author, log_ratio))+
  geom_violin()+
  geom_hline(aes(yintercept = mean), linetype = 2)+
  geom_point(aes(y = mean), color = "red", size = 5)+
  scale_y_continuous(breaks = c(-7, -5, -3, -6.34))
```

Красной точкой обозначены средние значения, так что мы видим, что между двумя писателями есть разница, но является ли она статистически значимой? В прошлом разделе мы рассмотрели, что в таком случае можно сделать t-test:

```{r}
t.test(log_ratio~author, 
       data = checkov_zoshenko, 
       var.equal = TRUE) # здесь я мухлюю, отключая поправку Уэлча
```

Разница между группами является статистически значимой (t(125) = 5.6871, p-value = 8.665e-08).

Для того, чтобы запустить регрессию на категориальных данных, категориальная переменная автоматически разбивается на группу бинарных dummy-переменных:

```{r}
tibble(author = c("Чехов", "Зощенко"),
       dummy_chekhov = c(1, 0),
       dummy_zoshenko = c(0, 1))
```

Дальше для регрессионного анализа выкидывают одну из переменных, так как иначе модель не сойдется (dummy-переменных всегда n-1, где n --- количество категорий в переменной). 

```{r}
tibble(author = c("Чехов", "Зощенко"),
       dummy_chekhov = c(1, 0))
```

Если переменная `dummy_chekhov` принимает значение 1, значит речь о рассказе Чехова, а если принимает значение 0, то о рассказе Зощенко. Если вставить нашу переменную в регрессионную формулу, получится следующее:

$$y_i = \hat\beta_0 + \hat\beta_1 \times \text{dummy\_chekhov} + \epsilon_i$$

Так как  `dummy_chekhov` принимает либо значение 1, либо значение 0, то получается, что модель предсказывает лишь два значения:

$$y_i = \left\{\begin{array}{ll}\hat\beta_0 + \hat\beta_1 \times 1 + \epsilon_i = \hat\beta_0 + \hat\beta_1 + \epsilon_i\text{, если рассказ Чехова}\\ 
\hat\beta_0 + \hat\beta_1 \times 0 + \epsilon_i = \hat\beta_0 + \epsilon_i\text{, если рассказ Зощенко}
\end{array}\right.$$

Таким образом, получается, что свободный член $\beta_0$ и угловой коэффициент $\beta_1$ в регрессии с категориальной переменной получают другую интерпретацию. Одно из значений переменной кодируется при помощи $\beta_0$, а сумма коэффициентов $\beta_0+\beta_1$ дает другое значение переменной. Так что $\beta_1$ --- это разница между оценками двух значений переменной.

Давайте теперь запустим регрессию на этих же данных:

```{r}
fit2 <- lm(log_ratio~author, data = checkov_zoshenko)
summary(fit2)
```

Во-первых, стоит обратить внимание на то, что R сам преобразовал нашу категориальную переменную в dummy-переменную `authorЧехов`. Во-вторых, можно заметить, что значения t-статистики и p-value совпадают с результатами, полученными нами в t-тесте выше. Статистически значимый коэффициент при аргументе `authorЧехов` следует интерпретировать как разницу средних между логарифмом долей в рассказах Чехова и Зощенко.

### Множественная регрессия

Множественная регрессия позволяет проанализировать связь между переменной и несколькими предикторами. Формула множественной регрессии не сильно отличается от формулы обычной линейной регрессии:

$$y_i = \hat\beta_0 + \hat\beta_1 \times x_{1i}+ \dots+ \hat\beta_n \times x_{ni} + \epsilon_i,$$

* $x_{ki}$ --- $i$-ый элемент векторов значений $X_1, \dots, X_n$;
* $y_i$ --- $i$-ый элемент вектора значений $Y$;
* $\hat\beta_0$ --- оценка случайного члена (intercept);
* $\hat\beta_k$ --- коэфциент при переменной $X_{k}$;
* $\epsilon_i$ --- $i$-ый остаток, разница между оценкой модели ($\hat\beta_0 + \hat\beta_1 \times x_i$) и реальным значением $y_i$; весь вектор остатков иногда называют случайным шумом.

В такой регрессии предикторы могут быть как числовыми, так и категориальными (со всеми вытекающими последствиями, которые мы обсудили в предыдущем разделе). Такую регрессию чаще всего сложно визуализировать, так как в одну регрессионную линию вкладываются сразу несколько переменных.

Попробуем предсказать длину лепестка на основе длины чашелистиков и вида ириса:

```{r}
iris |> 
  ggplot(aes(Sepal.Length, Petal.Length, color = Species))+
  geom_point()
```

Запустим регрессию:

```{r}
fit3 <- lm(Petal.Length ~ Sepal.Length + Species, data = iris)
summary(fit3)
```

Все предикторы статистически значимы. Давайте посмотрим предсказания модели для всех наблюдений:

```{r}
iris |> 
  mutate(prediction = predict(fit3)) |> 
  ggplot(aes(Sepal.Length, prediction, color = Species))+
  geom_point()
```

Всегда имеет смысл визуализировать, что нам говорит наша модель. Если использовать пакет `ggeffects` (или предшествовавший ему пакет `effects`), это можно сделать не сильно задумываясь, как это делать:

```{r}
#| message: false
library(ggeffects)
plot(ggpredict(fit3, terms = c("Sepal.Length", "Species")))
```

Как видно из графиков, наша модель имеет одинаковые угловые коэффициенты (slope) для каждого из видов ириса и разные свободные члены (intercept).

```{r}
summary(fit3)
```

$$y_i = \left\{\begin{array}{ll} -1.70234 + 0.63211 \times \text{Sepal.Length} + \epsilon_i\text{, если вид setosa}\\ 
-1.70234 + 2.2101 + 0.63211 \times \text{Sepal.Length} + \epsilon_i\text{, если вид versicolor} \\
-1.70234 + 3.09 + 0.63211 \times \text{Sepal.Length} + \epsilon_i\text{, если вид virginica}
\end{array}\right.$$

### Сравнение моделей

Как нам решить, какая модель лучше? Ведь теперь можно добавить сколько угодно предикторов? Давайте создадим новую модель без предиктора `Species`:

```{r}
fit4 <- lm(Petal.Length ~ Sepal.Length, data = iris)
```

* можно сравнивать статистическую значимость предикторов
* можно сравнивать $R^2$
```{r}
summary(fit3)$adj.r.squared
summary(fit4)$adj.r.squared
```
* чаще всего используют так называемые информационные критерии, самый популярный -- AIC (Akaike information criterion). Само по себе значение этого критерия не имеет смысла -- только в сравнении моделей, построенных на похожих данных. Чем меньше значение, тем модель лучше.

```{r}
AIC(fit3, fit4)
```


### Послесловие

- существуют ограничения на применение линейной регрессии   
    - связь между предсказываемой переменной и предикторами должна быть линейной
    - остатки должны быть нормально распределены (оценивайте визуально)
    - дисперсия остатков вокруг регрессионной линии должна быть постоянна (гомоскедастичность)
    - предикторы не должны коррелировать друг с другом
    - все наблюдения в регрессии должны быть независимы друг от друга
    
Вот так вот выглядят остатки нашей модели на основе датасета `iris`. Смотрите [пост](https://www.qualtrics.com/support/stats-iq/analyses/regression-guides/interpreting-residual-plots-improve-regression/), в котором обсуждается, как интерпретировать график остатков.

```{r}
plot(fit3, which=c(1, 2))
```

- существуют трюки, позволяющие автоматически отбирать модели (см. функцию `step()`) 
- существует достаточно большое семейство регрессий, которые зависят от типа независимой (предсказываемой) переменной или ее распределения
    - логистическая (если предсказываемая переменная имеет два возможных исхода)
    - мультиномиальная (если предсказываемая переменная имеет больше двух возможных дискретных исходов)
    - нелинейные регрессии (если связь между переменными нелинейна)
    - регрессия со смешанными эффектами (если внутри данных есть группировки, т. е. наблюдения не независимы)
    - и другие.
